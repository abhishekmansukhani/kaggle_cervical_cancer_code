{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Experiments: Training the entire dense layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Using gpu device 0: Tesla K80 (CNMeM is disabled, cuDNN 5103)\n",
      "/home/ubuntu/anaconda2/lib/python2.7/site-packages/theano/sandbox/cuda/__init__.py:600: UserWarning: Your cuDNN version is more recent than the one Theano officially supports. If you see any problems, try updating Theano or downgrading cuDNN to version 5.\n",
      "  warnings.warn(warn)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pretrained Vgg16 model loaded.\n",
      "Found 6255 images belonging to 3 classes.\n",
      "Found 1549 images belonging to 3 classes.\n",
      "initialized training data from: ../data/full/train\n",
      "initialized validation data from: ../data/full/valid\n",
      "found number of softmax classes: 3\n"
     ]
    }
   ],
   "source": [
    "from Executor import *\n",
    "# Reset the model, and run same finetuning using precomputed conv. model outputs. See if they match...\n",
    "executor = ExecutorBuilder().\\\n",
    "    with_runID(\"first\").\\\n",
    "    and_().\\\n",
    "    with_Vgg16().\\\n",
    "    and_().\\\n",
    "    train_batch_size(128). \\\n",
    "    and_(). \\\n",
    "    learn_rate(0.001).\\\n",
    "    and_().\\\n",
    "    data_on_path(\"../data/full/\").\\\n",
    "    build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading precomputed conv. outputs...\n",
      "done...\n",
      "getting rescaled fc model...\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "found dense layer. Distributing scaled weights..\n",
      "done...\n",
      "('updating dropout from: ', 0.5, ' to: ', 0.5)\n",
      "('found trainable layer: ', <keras.layers.pooling.MaxPooling2D object at 0x7f499c0787d0>)\n",
      "('found trainable layer: ', <keras.layers.core.Flatten object at 0x7f495c2ef750>)\n",
      "('found trainable layer: ', <keras.layers.core.Dense object at 0x7f4959ab8ad0>)\n",
      "('found trainable layer: ', <keras.layers.core.Dropout object at 0x7f4959ab8690>)\n",
      "('found trainable layer: ', <keras.layers.core.Dense object at 0x7f4974af7d50>)\n",
      "('found trainable layer: ', <keras.layers.core.Dropout object at 0x7f4959b1de90>)\n",
      "('found trainable layer: ', <keras.layers.core.Dense object at 0x7f4959b1ded0>)\n"
     ]
    }
   ],
   "source": [
    "executor.load_model_from_file() # loads the model from the previous ipynb file\n",
    "executor.load_precomputed_conv_models()\n",
    "# Get the linear model \n",
    "linearModel = executor.get_rescaled_fc_model(new_dropout=executor.dropout)\n",
    "\n",
    "# Set all layers being trainable\n",
    "for layer in linearModel.layers:\n",
    "    if layer.trainable:\n",
    "        print(\"found trainable layer: \",layer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6255 samples, validate on 1549 samples\n",
      "Epoch 1/5\n",
      "6255/6255 [==============================] - 7s - loss: 1.9772 - acc: 0.3952 - val_loss: 1.1805 - val_acc: 0.4913\n",
      "Epoch 2/5\n",
      "6255/6255 [==============================] - 7s - loss: 1.7330 - acc: 0.4230 - val_loss: 1.0740 - val_acc: 0.5029\n",
      "Epoch 3/5\n",
      "6255/6255 [==============================] - 7s - loss: 1.5726 - acc: 0.4456 - val_loss: 1.0362 - val_acc: 0.5216\n",
      "Epoch 4/5\n",
      "6255/6255 [==============================] - 7s - loss: 1.5259 - acc: 0.4571 - val_loss: 0.9929 - val_acc: 0.5339\n",
      "Epoch 5/5\n",
      "6255/6255 [==============================] - 7s - loss: 1.3903 - acc: 0.4811 - val_loss: 0.9711 - val_acc: 0.5455\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4897a2dd90>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearModel.compile(optimizer=Adam(lr=0.000001), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "linearModel.fit(executor.train_precomputed, executor.train_labels, batch_size=executor.batch_size,\n",
    "                nb_epoch= 5, validation_data=(executor.val_precomputed, executor.val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now try to finetune the model, then maybe set the all dense layers trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6255 samples, validate on 1549 samples\n",
      "Epoch 1/5\n",
      "6255/6255 [==============================] - 7s - loss: 1.2040 - acc: 0.5133 - val_loss: 0.8693 - val_acc: 0.5875\n",
      "Epoch 2/5\n",
      "6255/6255 [==============================] - 7s - loss: 0.9192 - acc: 0.5859 - val_loss: 0.8246 - val_acc: 0.6133\n",
      "Epoch 3/5\n",
      "6255/6255 [==============================] - 7s - loss: 0.7683 - acc: 0.6512 - val_loss: 0.7899 - val_acc: 0.6294\n",
      "Epoch 4/5\n",
      "6255/6255 [==============================] - 7s - loss: 0.6616 - acc: 0.7165 - val_loss: 0.7564 - val_acc: 0.6630\n",
      "Epoch 5/5\n",
      "6255/6255 [==============================] - 7s - loss: 0.5715 - acc: 0.7604 - val_loss: 0.7274 - val_acc: 0.6766\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4892690610>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linearModel.compile(optimizer=Adam(lr=0.00001), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "linearModel.fit(executor.train_precomputed, executor.train_labels, batch_size=executor.batch_size,\n",
    "                nb_epoch= 5, validation_data=(executor.val_precomputed, executor.val_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6255 samples, validate on 1549 samples\n",
      "Epoch 1/5\n",
      "6255/6255 [==============================] - 7s - loss: 0.8848 - acc: 0.5906 - val_loss: 0.7973 - val_acc: 0.5997\n",
      "Epoch 2/5\n",
      "6255/6255 [==============================] - 7s - loss: 0.6228 - acc: 0.7162 - val_loss: 0.7866 - val_acc: 0.6475\n",
      "Epoch 3/5\n",
      "6255/6255 [==============================] - 7s - loss: 0.3915 - acc: 0.8374 - val_loss: 0.7836 - val_acc: 0.6888\n",
      "Epoch 4/5\n",
      "6255/6255 [==============================] - 7s - loss: 0.2783 - acc: 0.8878 - val_loss: 0.9418 - val_acc: 0.6940\n",
      "Epoch 5/5\n",
      "6255/6255 [==============================] - 7s - loss: 0.2044 - acc: 0.9263 - val_loss: 0.8864 - val_acc: 0.7114\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4896ecf290>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# BAD IDEA... to increase the learn_rate, if anything, the learn_rate should be decreased.\n",
    "linearModel.compile(optimizer=Adam(lr=0.0001), loss='categorical_crossentropy', metrics = ['accuracy'])\n",
    "linearModel.fit(executor.train_precomputed, executor.train_labels, batch_size=executor.batch_size,\n",
    "                nb_epoch= 5, validation_data=(executor.val_precomputed, executor.val_labels))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
